{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kdavis2025/Projects/blob/main/Project_5_NLP_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kw6xOpoV1SV"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "This project will give you practical experience using Natural Language Processing techniques. This project is in three parts:\n",
        "\n",
        "1. in part 1) you will use a dataset in a CSV file\n",
        "2. in part 2) you will use the Wikipedia API to directly access content on Wikipedia.\n",
        "3. in part 3) you will make your notebook interactive\n",
        "\n",
        "**Part 1**\n",
        "- The CSV file is available at https://ddc-datascience.s3.amazonaws.com/Projects/Project.5-NLP/Data/NLP.csv\n",
        "- The file contains a list of famous people and a brief overview.\n",
        "- The goal of part 1) is to:\n",
        "1. Pick one person from the list ( the target person ) and output 10 other people who's overview are \"closest\" to the target person in a Natural Language Processing sense\n",
        "2. Also output the sentiment of the overview of the target person.\n",
        "**Part 2**\n",
        "- For the same target person that you chose in Part 1), use the Wikipedia API to access the whole content of the target person's Wikipedia page.\n",
        "- The goal of Part 2) is to ...\n",
        "1. Print out the text of the Wikipedia article for the target person\n",
        "2. Determine the sentiment of the text of the Wikipedia page for the target person\n",
        "3. Collect the text of the Wikipedia pages from the 10 nearest neighbors from Part 1)\n",
        "4. Determine the nearness ranking of these 10 people to your target person based on their entire Wikipedia page\n",
        "5. Compare, i.e. plot, the nearest ranking from Step 1) with the Wikipedia page nearness ranking. A difference of the rank is one means of comparison.\n",
        "**Part 3**\n",
        "- Make an interactive notebook where a user can choose or enter a name and the notebook displays the 10 closest individuals.\n",
        "\n",
        "- In addition to presenting the project slides, at the end of the presentation each student will demonstrate their code using a famous person suggested by the other students that exists in the DBpedia set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLMi3HgWSiTF"
      },
      "source": [
        "## Business Case: Problem Definition\n",
        "\n",
        "Business Case Problem Definition\n",
        "Marketing/PR teams, talent agencies, and content platforms need a fast, scalable way to identify which public figures are thematically similar and understand how they’re portrayed. Today, analysts manually review bios and disparate sentiment sources, a slow process that hinders campaign agility and partnership decisions. **The goal is to automate discovery of the top-10 closest personalities to any target, surface standardized sentiment scores (polarity and subjectivity), compare “short-form” vs. “long-form” similarity, and deliver insights via an intuitive interface—thereby cutting research time by up to 80%, improving targeting accuracy, and enabling data-driven talent strategy.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BsTNS3SSrRk"
      },
      "source": [
        "## Data Science Problem Definition\n",
        "\n",
        "**This is a Unsupervised Modeling by use of Similarity Retrieval**: Use cosine-based NearestNeighbors to rank the top-10 most similar figures in each space.\n",
        "\n",
        "1. Data Ingestion & Cleaning: Load a CSV of names/overviews and fetch full Wikipedia articles via API. Preprocess text by lowercasing, stripping punctuation/possessives, and removing stopwords.\n",
        "\n",
        "2. Feature Engineering: Build TF-IDF vector spaces for both the overview corpus and full-text corpus.\n",
        "\n",
        "3. Sentiment Analysis: Apply TextBlob to compute polarity and subjectivity for both overview and full-page texts.\n",
        "\n",
        "4. Evaluation & Visualization: Quantify alignment between overview- and full-text rankings via Spearman’s ρ, display rank-difference plots, and deploy an interactive widget for sub-second queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRagsSoySpsZ"
      },
      "source": [
        "## Data Overview\n",
        "\n",
        "1. **Columns:** 3 (URI, name, text)\n",
        "2. **Entries:** 42,786 (0 to 42,785)\n",
        "3. **Data Types:** 3 (Objects)\n",
        "3. **File Size:** 1002.9 KB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThREl_crU_2s"
      },
      "source": [
        "## Data Collection/Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx0cejuPwKSv"
      },
      "source": [
        "Prepare Proper Installs for Project Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWEcxzkMplbm",
        "outputId": "33769e08-8fcf-4908-8051-9add2c4a7dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#---------Install all required libraries (Combined)-------------------------#\n",
        "# wikipedia-api: Used to interact with and extract data from Wikipedia.\n",
        "# wikipedia: Another library to access Wikipedia data.\n",
        "# textblob: A library for performing common NLP tasks, including sentiment analysis.\n",
        "# nltk: (Natural Language Toolkit) A comprehensive library for various NLP tasks, such as tokenization and part-of-speech tagging.\n",
        "# scikit-learn: A machine learning library that includes tools for tasks like\n",
        "!pip install --quiet wikipedia-api wikipedia textblob nltk scikit-learn ipywidgets matplotlib\n",
        "\n",
        "#---------Import Implementation----------------------------------------------#\n",
        "# Used for working with regular expressions (a way to find and manipulate patterns in text).\n",
        "# Used for numerical computations and working with arrays of numbers.\n",
        "# Used for data manipulation and analysis. It's particularly handy for working with data in tables (like the CSV file used in this project).\n",
        "# Used for data visualization and plotting comparisons\n",
        "# Used for wikipedia fetch, sentiment & ranking comparison\n",
        "# Converts text into numerical representations that machine learning algorithms can understand. TF-IDF stands for \"Term Frequency-Inverse Document Frequency\" and is a common way to represent text data.\n",
        "# Used to find data points that are similar to each other (like finding famous people with similar overviews).\n",
        "# Used for common Natural Language Processing tasks like sentiment analysis (figuring out if a piece of text is positive, negative, or neutral).\n",
        "# Used to decode special characters in URLs (web addresses).\n",
        "# This function automatically creates user interface (UI) controls (like sliders, dropdowns, etc.) that are linked to a Python function. When the user interacts with the UI controls, the function is automatically re-run with the new values, making the notebook dynamic. Provides the building blocks for the UI controls themselves. Buttons, Text boxes, Sliders, etc., are all types of widgets.\n",
        "# Clears the output of a previously executed code cell. Helpful when you're building interactive elements, as it lets you replace old results with new ones without cluttering the notebook.\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import wikipediaapi\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from textblob import TextBlob\n",
        "from urllib.parse import unquote\n",
        "from ipywidgets import interact, widgets\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#-------- Download necessary NLTK data---------------------------------------#\n",
        "# Provides a pre-trained sentence tokenizer, used to split text into sentences.\n",
        "# Provides a pre-trained part-of-speech tagger, used to identify the grammatical role of words in a sentence.\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#-------- Download TextBlob corpora (sentiment lexicons)---------------------#\n",
        "!python3 -m textblob.download_corpora >/dev/null\n",
        "\n",
        "# Enable inline plotting\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejSoKfXwwe7h"
      },
      "source": [
        "Part 1: Load Data & Compute Nearest Neighbors on Overviews Section of Manny Pacquiao"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JmBECtuS0lp",
        "outputId": "4ef749c6-31a3-4484-f686-3fa9799ba735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42786 entries, 0 to 42785\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   URI     42786 non-null  object\n",
            " 1   name    42786 non-null  object\n",
            " 2   text    42786 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1002.9+ KB\n"
          ]
        }
      ],
      "source": [
        "# 1. Load the CSV of famous people and their overviews\n",
        "DATA_URL = 'https://ddc-datascience.s3.amazonaws.com/Projects/Project.5-NLP/Data/NLP.csv'\n",
        "df = pd.read_csv(DATA_URL)\n",
        "\n",
        "#------------ Painpoint, output as: H%C3%BClya %C5%9Eahin-------------------------------#\n",
        "# decode any percent-encoded names (e.g. H%C3%BClya %C5%9Eahin → Hülya Şahin)\n",
        "df['name'] = df['name'].apply(unquote)\n",
        "\n",
        "# Gather info on the Data\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqhKDPMoDpdC"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4K-oqcLL4am"
      },
      "source": [
        "Note: Its best to create function to implement program for simpler use of GUI in Part 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIrn7Kjgpuee"
      },
      "outputs": [],
      "source": [
        "# 2. Preprocess function: lowercase, remove newlines, possessives, punctuation\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
        "    text = re.sub(r\"\\'s\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "# 3. Apply cleaning to the overview text\n",
        "df['clean_text'] = df['text'].fillna('').apply(preprocess)\n",
        "\n",
        "# 4. Vectorize all overviews using TF-IDF (English stopwords removed)\n",
        "tfidf_vect = TfidfVectorizer(stop_words='english')\n",
        "X_overviews = tfidf_vect.fit_transform(df['clean_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW99ht2eD_o1"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHen8Xi9FtsL",
        "outputId": "0f5569a7-b0e5-4816-bb3f-f867f2eb3ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 nearest neighbors to Manny Pacquiao based on overview TF-IDF:\n",
            "\n",
            " 1. Bernard Hopkins  (cosine distance: 0.7745)\n",
            " 2. Kaliesha West  (cosine distance: 0.7758)\n",
            " 3. Hülya Şahin  (cosine distance: 0.7803)\n",
            " 4. Danny Ildefonso  (cosine distance: 0.7907)\n",
            " 5. Pernell Whitaker  (cosine distance: 0.7928)\n",
            " 6. Curt McCune  (cosine distance: 0.7947)\n",
            " 7. Mario Yagobi  (cosine distance: 0.8022)\n",
            " 8. Marvelous Marvin Hagler  (cosine distance: 0.8067)\n",
            " 9. Haider Ali (boxer)  (cosine distance: 0.8072)\n",
            "10. Miguel Cotto  (cosine distance: 0.8073)\n",
            "\n",
            "Sentiment for Manny Pacquiao overview:\n",
            "  • Polarity   = 0.226\n",
            "  • Subjectivity = 0.333\n"
          ]
        }
      ],
      "source": [
        "# 5. Fit a NearestNeighbors model (cosine similarity)\n",
        "nn_overview = NearestNeighbors(metric='cosine')\n",
        "nn_overview.fit(X_overviews)\n",
        "\n",
        "# 6. Identify the index for our target person (Manny Pacquiao)\n",
        "target_name = 'Manny Pacquiao'\n",
        "if target_name not in df['name'].values:\n",
        "    raise ValueError(f\"{target_name} not found in dataset\") #Exception handling for name unfound\n",
        "target_idx = int(df.index[df['name'] == target_name][0]) # Target Index beggining location.\n",
        "\n",
        "# 7. Query the 11 nearest (including self), then drop the first (itself)\n",
        "distances, indices = nn_overview.kneighbors(\n",
        "    X_overviews[target_idx], n_neighbors=11 # 0-10 with Manny Pacquiao included for analysis\n",
        ")\n",
        "neighbor_idxs = indices.flatten()[1:] # indices stores the index within the dataset of each neighbor with MP removed.\n",
        "neighbor_dists = distances.flatten()[1:] # indices stores the distances within the dataset of each neighbor with MP removed.\n",
        "\n",
        "# 8. Display the 10 nearest neighbors with distances\n",
        "print(f\"10 nearest neighbors to {target_name} based on overview TF-IDF:\\n\")\n",
        "for rank, (idx, dist) in enumerate(zip(neighbor_idxs, neighbor_dists), start=1):\n",
        "    print(f\"{rank:2d}. {df.at[idx, 'name']}  (cosine distance: {dist:.4f})\")\n",
        "\n",
        "# 9. Sentiment analysis of Manny Pacquiao's overview\n",
        "manny_overview = df.at[target_idx, 'text']\n",
        "manny_sent = TextBlob(manny_overview).sentiment\n",
        "print(f\"\\nSentiment for {target_name} overview:\\n\"\n",
        "      f\"  • Polarity   = {manny_sent.polarity:.3f}\\n\"\n",
        "      f\"  • Subjectivity = {manny_sent.subjectivity:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83rRUNVtEtCW"
      },
      "source": [
        "## Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8yepCxWHTGp"
      },
      "source": [
        "Part 2: Wikipedia Fetch, Sentiment & Ranking Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "L0uSY_j0FDng",
        "outputId": "4f8f7035-b21c-4e68-db8a-0f86d32d5007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full-page sentiment for Manny Pacquiao:\n",
            "  • Polarity=0.102, Subjectivity=0.393\n",
            "\n",
            "Full-page sentiment for each neighbor:\n",
            "  • Bernard Hopkins: Polarity=0.141, Subjectivity=0.413\n",
            "  • Kaliesha West: Polarity=0.119, Subjectivity=0.162\n",
            "  • Hülya Şahin: Polarity=0.072, Subjectivity=0.336\n",
            "  • Danny Ildefonso: Polarity=0.078, Subjectivity=0.400\n",
            "  • Pernell Whitaker: Polarity=0.136, Subjectivity=0.403\n",
            "  • Curt McCune: Polarity=0.036, Subjectivity=0.289\n",
            "  • Mario Yagobi: Polarity=0.121, Subjectivity=0.291\n",
            "  • Marvelous Marvin Hagler: Polarity=0.099, Subjectivity=0.401\n",
            "  • Haider Ali (boxer): Polarity=0.141, Subjectivity=0.268\n",
            "  • Miguel Cotto: Polarity=0.089, Subjectivity=0.390\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TfidfVectorizer' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f0b771f99c68>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mfull_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcorpus_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_order_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtfidf_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "# 1. Initialize the Wikipedia API client\n",
        "wiki = wikipediaapi.Wikipedia(language='en', user_agent='NLP-Project')\n",
        "\n",
        "# 2. Helper: fetch & clean full Wikipedia page text for a person\n",
        "def fetch_wiki_text(name):\n",
        "    page = wiki.page(name)\n",
        "    if not page.exists():\n",
        "        return \"\"\n",
        "    text = page.text\n",
        "    # reuse our preprocess for basic cleaning\n",
        "    return preprocess(text)\n",
        "\n",
        "# 3. Fetch full page text for Manny and the 10 neighbors\n",
        "neighbors = df.loc[neighbor_idxs, 'name'].tolist()\n",
        "target_full = fetch_wiki_text(target_name)\n",
        "neighbors_full = [fetch_wiki_text(n) for n in neighbors]\n",
        "\n",
        "# 4. Sentiment on full pages\n",
        "print(f\"Full-page sentiment for {target_name}:\")\n",
        "ts = TextBlob(target_full).sentiment\n",
        "print(f\"  • Polarity={ts.polarity:.3f}, Subjectivity={ts.subjectivity:.3f}\\n\")\n",
        "\n",
        "print(\"Full-page sentiment for each neighbor:\")\n",
        "for name, text in zip(neighbors, neighbors_full):\n",
        "    s = TextBlob(text).sentiment\n",
        "    print(f\"  • {name}: Polarity={s.polarity:.3f}, Subjectivity={s.subjectivity:.3f}\")\n",
        "\n",
        "# 5. Build a TF-IDF corpus on the 11 full texts, then re-run NearestNeighbors\n",
        "corpus = [target_full] + neighbors_full\n",
        "tfidf_full = TfidfVectorizer(stop_words='english', min_df=1)\n",
        "X_full = tfidf_full.fit_transform(corpus)\n",
        "\n",
        "nn_full = NearestNeighbors(metric='cosine')\n",
        "nn_full.fit(X_full)\n",
        "dist_full, idx_full = nn_full.kneighbors(X_full[0], n_neighbors=11)\n",
        "\n",
        "# 6. Extract full-page neighbor order (skip self at position 0)\n",
        "full_order_idxs = idx_full.flatten()[1:]\n",
        "full_order = [corpus[i] for i in full_order_idxs]  # not used directly\n",
        "full_names = [ [target_name] + neighbors ][0]  # helper list\n",
        "\n",
        "# Actually build full-page neighbor names:\n",
        "corpus_names = [target_name] + neighbors\n",
        "full_neighbors = [corpus_names[i] for i in full_order_idxs]\n",
        "\n",
        "tfidf_full.shape()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlc-Ks6LEKYj"
      },
      "source": [
        "## Data Visualization/Communication of Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBt4pIxXGpvd"
      },
      "outputs": [],
      "source": [
        "# 7. Assemble ranking comparison\n",
        "overview_ranks = {name: r for r,name in enumerate(neighbors, start=1)}\n",
        "fullpage_ranks = {name: r for r,name in enumerate(full_neighbors, start=1)}\n",
        "\n",
        "ranking_df = pd.DataFrame({\n",
        "    'name': neighbors,\n",
        "    'overview_rank': [overview_ranks[n] for n in neighbors],\n",
        "    'fullpage_rank': [fullpage_ranks.get(n, None) for n in neighbors]\n",
        "})\n",
        "ranking_df['rank_diff'] = ranking_df['overview_rank'] - ranking_df['fullpage_rank']\n",
        "\n",
        "print(\"\\nRank comparison table:\")\n",
        "display(ranking_df)\n",
        "\n",
        "# 8. Plot line graph of overview vs full-page ranks side by side\n",
        "plt.figure(figsize=(10,6))\n",
        "x = np.arange(len(neighbors)) + 1\n",
        "plt.plot(x, ranking_df['overview_rank'],   marker='o', label='Overview Rank')\n",
        "plt.plot(x, ranking_df['fullpage_rank'],   marker='x', label='Full-page Rank')\n",
        "plt.xticks(x, ranking_df['name'], rotation=45, ha='right')\n",
        "plt.xlabel('Neighbor')\n",
        "plt.ylabel('Rank (1 = closest)')\n",
        "plt.title('Nearest Neighbor Rank: Overview vs Full Wikipedia Page')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyNl0bNyrsk_"
      },
      "source": [
        "Part 3: Interactive Selection of Any Person"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4umQZYBlpuV3"
      },
      "outputs": [],
      "source": [
        "# 1. Define a function to fetch & display top-N neighbors for any selected name\n",
        "def show_neighbors(selected_name: str, k: int = 10):\n",
        "    clear_output(wait=True)  # clear previous output\n",
        "    if selected_name not in df['name'].values:\n",
        "        print(f\"❌  '{selected_name}' not found in dataset.\")\n",
        "        return\n",
        "\n",
        "#-------- Can use sentiment analsis rather than distance---------------#\n",
        "    idx = int(df.index[df['name'] == selected_name][0])\n",
        "    dists, idxs = nn_overview.kneighbors(X_overviews[idx], n_neighbors=k+1)\n",
        "\n",
        "    print(f\"Top {k} neighbors for '{selected_name}':\\n\")\n",
        "    for i, (nbr_idx, dist) in enumerate(zip(idxs.flatten()[1:], dists.flatten()[1:]), start=1): # ouc\n",
        "            print(f\"{i:2d}. {df.at[nbr_idx, 'name']} (distance: {dist:.4f})\")\n",
        "\n",
        "# 2. Build a dropdown widget of all names\n",
        "name_widget = widgets.Combobox(\n",
        "    placeholder='Type or select a name',\n",
        "    options=sorted(df['name'].tolist()),\n",
        "    description='Person:',\n",
        "    ensure_option=True,\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "\n",
        "# 3. Tie it together with interact\n",
        "interact(show_neighbors,\n",
        "         selected_name=name_widget,\n",
        "         k=widgets.IntSlider(value=10, min=5, max=20, step=1, description='# Neighbors:'));"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPG4oZYXUS+U/zmaLzVhR9j",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}